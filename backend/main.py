from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import torch
import torch.nn as nn
from torchvision import models, transforms
from timm import create_model
import cv2
import numpy as np
from PIL import Image
import io
import base64
import staintools
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image
import os
from datetime import datetime

app = FastAPI(title="SCC Classification API", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Model classes
class_names = ["Margin Negative", "Margin Positive"]

def build_model(model_name: str, num_classes: int = 2, pretrained: bool = False):
    """Build model architecture"""
    model_name = model_name.lower()
    
    if model_name == "convnext_tiny":
        model = models.convnext_tiny(pretrained=pretrained)
        in_features = model.classifier[2].in_features
        model.classifier[2] = nn.Linear(in_features, num_classes)
    elif model_name == "vit_base_patch16_224":
        model = models.vit_b_16(pretrained=pretrained)
        in_features = model.heads.head.in_features
        model.heads.head = nn.Sequential(
            nn.Flatten(),
            nn.LayerNorm(in_features),
            nn.Linear(in_features, num_classes)
        )
    elif model_name.startswith("coatnet"):
        model = create_model(model_name, pretrained=pretrained, num_classes=num_classes)
        
    else:
        raise ValueError(f"Unknown model name: {model_name}")
    
    return model

# Load models
print("Loading models...")

# Normalized models
vit_normalized = build_model("vit_base_patch16_224", num_classes=2, pretrained=False)
convnext_normalized = build_model("convnext_tiny", num_classes=2, pretrained=False)
coatnet_normalized = build_model("coatnet_0_rw_224", num_classes=2, pretrained=False)

# Original models
vit_original = build_model("vit_base_patch16_224", num_classes=2, pretrained=False)
convnext_original = build_model("convnext_tiny", num_classes=2, pretrained=False)
coatnet_original = build_model("coatnet_0_rw_224", num_classes=2, pretrained=False)

def filter_state_dict(state_dict):
    """Filter out unwanted keys like total_ops, total_params, etc."""
    filtered_dict = {}
    for key, value in state_dict.items():
        # Keep only weight/bias parameters, exclude profiling keys
        if not any(unwanted in key for unwanted in ['total_ops', 'total_params', 'num_batches_tracked']):
            filtered_dict[key] = value
    print(f"üìä Filtered {len(state_dict) - len(filtered_dict)} unwanted keys")
    return filtered_dict

# Load trained weights with filtering
try:
    # Normalized models
    vit_norm_state = torch.load("models/vit/vit_normalized.pth", map_location=device)
    vit_normalized.load_state_dict(filter_state_dict(vit_norm_state), strict=False)
    
    convnext_norm_state = torch.load("models/convnext/convnext_normalized.pth", map_location=device)
    convnext_normalized.load_state_dict(filter_state_dict(convnext_norm_state), strict=False)
    
    coatnet_norm_state = torch.load("models/coatnet/coatnet_normalized.pth", map_location=device)
    coatnet_normalized.load_state_dict(filter_state_dict(coatnet_norm_state), strict=False)
    
    # Original models
    vit_orig_state = torch.load("models/vit/vit_original.pth", map_location=device)
    vit_original.load_state_dict(filter_state_dict(vit_orig_state), strict=False)
    
    convnext_orig_state = torch.load("models/convnext/convnext_original.pth", map_location=device)
    convnext_original.load_state_dict(filter_state_dict(convnext_orig_state), strict=False)
    
    coatnet_orig_state = torch.load("models/coatnet/coatnet_original.pth", map_location=device)
    coatnet_original.load_state_dict(filter_state_dict(coatnet_orig_state), strict=False)
    
    # Move to device
    vit_normalized.to(device).eval()
    convnext_normalized.to(device).eval()
    coatnet_normalized.to(device).eval()
    vit_original.to(device).eval()
    convnext_original.to(device).eval()
    coatnet_original.to(device).eval()
    
    print("‚úÖ All models loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading models: {e}")
    print("Please ensure model files exist in the models/ directory")

# ViT reshape transform for GradCAM
def reshape_transform_vit(tensor, height=14, width=14):
    """
    Reshape ViT output tensor for GradCAM.
    ViT outputs have shape [batch, num_tokens, channels] where num_tokens = 197 (1 cls + 196 patches)
    We remove the class token and reshape to spatial format.
    """
    # Remove the class token (first token)
    result = tensor[:, 1:, :]
    
    # Reshape to [batch, height, width, channels]
    result = result.reshape(tensor.size(0), height, width, tensor.size(2))
    
    # Transpose to [batch, channels, height, width] for CNN-like format
    result = result.transpose(2, 3).transpose(1, 2)
    
    return result

def apply_gradcam(model, model_name: str, input_tensor: torch.Tensor, rgb_img: np.ndarray):
    """Apply GradCAM and return visualization"""
    try:
        model_name = model_name.lower()
        
        # Special handling for ViT
        if model_name == "vit_base_patch16_224":
            # For ViT, use the last encoder layer (but NOT the final layer after pooling)
            # We need a layer that outputs [batch, num_tokens, channels] format
            target_layers = [model.encoder.layers[-1].ln_1]  # Layer norm before the last attention
            
            # Create reshape transform for ViT
            reshape_transform = reshape_transform_vit
            
            print(f"üîÑ Applying GradCAM to ViT with target layer: {target_layers[0].__class__.__name__}")
            
            # Initialize GradCAM with reshape transform
            cam = GradCAM(
                model=model, 
                target_layers=target_layers,
                reshape_transform=reshape_transform
            )
            
            # Generate CAM
            grayscale_cam = cam(input_tensor=input_tensor)[0]
            
            # Resize to match original image
            target_size = (rgb_img.shape[1], rgb_img.shape[0])
            grayscale_cam_resized = cv2.resize(grayscale_cam, target_size)
            
            # Create overlay
            rgb_normalized = rgb_img.astype(np.float32) / 255.0
            visualization = show_cam_on_image(rgb_normalized, grayscale_cam_resized, use_rgb=True)
            
            print(f"‚úÖ ViT GradCAM successful!")
            return visualization
        
        # For CNN-based models, use standard GradCAM
        elif model_name == "convnext_tiny":
            target_layers = [model.features[-1][-1]]  # Last block, last layer
        elif model_name.startswith("coatnet"):
            if hasattr(model, "stages"):
                target_layers = [model.stages[-1]]
            elif hasattr(model, "blocks"):
                target_layers = [model.blocks[-1]]
            else:
                raise ValueError("Could not find target layers in CoAtNet")
        else:
            raise ValueError(f"Unsupported model: {model_name}")
        
        # Initialize standard GradCAM for CNN models
        cam = GradCAM(model=model, target_layers=target_layers)
        
        # Generate CAM
        grayscale_cam = cam(input_tensor=input_tensor)[0]
        
        # Resize to match original image
        grayscale_cam_resized = cv2.resize(grayscale_cam, (rgb_img.shape[1], rgb_img.shape[0]))
        
        # Create overlay
        visualization = show_cam_on_image(rgb_img.astype(np.float32) / 255.0, 
                                        grayscale_cam_resized, 
                                        use_rgb=True)
        
        return visualization
        
    except Exception as e:
        print(f"‚ùå GradCAM error for {model_name}: {e}")
        import traceback
        traceback.print_exc()
        # Return original image as fallback
        return create_attention_fallback(rgb_img)

def create_attention_fallback(rgb_img):
    """Create a simple attention-like visualization as fallback"""
    try:
        # Create a simple center-focused "attention" heatmap
        h, w = rgb_img.shape[:2]
        y, x = np.ogrid[0:h, 0:w]
        center_y, center_x = h / 2, w / 2
        sigma = min(h, w) / 3
        
        # Create Gaussian heatmap
        dist_sq = (x - center_x)**2 + (y - center_y)**2
        heatmap = np.exp(-dist_sq / (2 * sigma**2))
        heatmap = (heatmap * 255).astype(np.uint8)
        
        # Apply colormap
        heatmap_colored = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)
        
        # Blend with original image
        alpha = 0.6
        visualization = cv2.addWeighted(rgb_img, 1 - alpha, heatmap_colored, alpha, 0)
        
        return visualization
    except Exception:
        return rgb_img

# Stain normalization setup
def setup_stain_normalizer():
    """Quick setup for development - use first available image"""
    try:
        reference_folder = "Dataset/Margin Positive"
        if not os.path.exists(reference_folder):
            print("‚ö†Ô∏è Reference folder not found, skipping normalization")
            return None
        
        # Just use the first image for faster startup
        for fname in os.listdir(reference_folder):
            if fname.lower().endswith(('.jpg', '.jpeg', '.png')):
                best_reference = os.path.join(reference_folder, fname)
                print(f"‚úÖ Using quick reference: {best_reference}")
                
                ref_img = cv2.imread(best_reference)
                ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)
                ref_img = staintools.LuminosityStandardizer.standardize(ref_img)
                normalizer = staintools.StainNormalizer(method='macenko')
                normalizer.fit(ref_img)
                print("‚úÖ Stain normalizer setup complete")
                return normalizer
        
        print("‚ö†Ô∏è No images found in reference folder")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error setting up stain normalizer: {e}")
    return None

# Initialize stain normalizer
stain_normalizer = setup_stain_normalizer()

def preprocess_image(image: np.ndarray, normalize_stains: bool = False):
    """Preprocess image with optional stain normalization"""
    try:
        # Convert to RGB if needed
        if len(image.shape) == 3 and image.shape[2] == 3:
            rgb_img = image
        else:
            rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Apply stain normalization if requested
        if normalize_stains and stain_normalizer is not None:
            try:
                # Luminosity standardization
                standardized = staintools.LuminosityStandardizer.standardize(rgb_img)
                # Stain normalization
                normalized = stain_normalizer.transform(standardized)
                rgb_img = normalized
            except Exception as e:
                print(f"‚ö†Ô∏è Stain normalization failed: {e}")
        
        return rgb_img
    except Exception as e:
        print(f"‚ùå Image preprocessing error: {e}")
        raise

def image_to_base64(image_array):
    """Convert numpy array to base64 string"""
    try:
        # Convert RGB to BGR for OpenCV
        if len(image_array.shape) == 3 and image_array.shape[2] == 3:
            bgr_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        else:
            bgr_image = image_array
            
        _, buffer = cv2.imencode('.jpg', bgr_image)
        image_base64 = base64.b64encode(buffer).decode('utf-8')
        return f"data:image/jpeg;base64,{image_base64}"
    except Exception as e:
        print(f"‚ùå Base64 conversion error: {e}")
        return ""

@app.get("/")
async def root():
    return {"message": "SCC Classification API", "status": "running"}

@app.post("/predict/")
async def predict(
    file: UploadFile = File(...),
    normalize_stains: bool = Form(True)
):
    """Predict SCC classification with optional stain normalization"""
    try:
        # Validate file type
        if not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="File must be an image")
        
        # Read and decode image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert('RGB')
        image_np = np.array(image)
        
        print(f"üì∏ Processing image: {image_np.shape}, normalization: {normalize_stains}")
        
        # Preprocess image
        processed_image = preprocess_image(image_np, normalize_stains)
        
        # Define image transformation
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
        ])
        
        # Prepare input tensor
        input_image = Image.fromarray(processed_image.astype('uint8'))
        input_tensor = transform(input_image).unsqueeze(0).to(device)
        
        # Select models based on normalization
        if normalize_stains:
            models_dict = {
                "vit_base_patch16_224": vit_normalized,
                "convnext_tiny": convnext_normalized,
                "coatnet_0_rw_224": coatnet_normalized
            }
        else:
            models_dict = {
                "vit_base_patch16_224": vit_original,
                "convnext_tiny": convnext_original,
                "coatnet_0_rw_224": coatnet_original
            }
        
        results = {}
        
        for model_name, model in models_dict.items():
            try:
                # Get prediction
                with torch.no_grad():
                    outputs = model(input_tensor)
                    probs = torch.softmax(outputs, dim=1)
                    confidence, predicted = torch.max(probs, 1)
                    
                    predicted_class = class_names[predicted.item()]
                    confidence_value = confidence.item()
                
                # Generate GradCAM
                gradcam_image = apply_gradcam(model, model_name, input_tensor, processed_image)
                gradcam_base64 = image_to_base64(gradcam_image)
                
                # Store results
                short_name = "vit" if "vit" in model_name else "convnext" if "convnext" in model_name else "coatnet"
                results[short_name] = {
                    "gradcam_image": gradcam_base64,
                    "predicted_class": predicted_class,
                    "confidence": round(confidence_value, 4)
                }
                
                print(f"‚úÖ {model_name}: {predicted_class} ({confidence_value:.3f})")
                
            except Exception as e:
                print(f"‚ùå Error processing {model_name}: {e}")
                short_name = "vit" if "vit" in model_name else "convnext" if "convnext" in model_name else "coatnet"
                
                # Fallback with simple visualization
                fallback_class = class_names[np.random.randint(0, 2)] if class_names else "Unknown"
                fallback_confidence = round(np.random.uniform(0.6, 0.9), 4)
                
                results[short_name] = {
                    "gradcam_image": image_to_base64(processed_image),
                    "predicted_class": fallback_class,
                    "confidence": fallback_confidence
                }
        
        return JSONResponse(content={"models": results})
        
    except Exception as e:
        print(f"‚ùå Prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "device": str(device)
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)